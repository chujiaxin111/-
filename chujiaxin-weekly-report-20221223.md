本周主要工作内容：

    机器学习课程的一部分

下周主要工作内容：

    完成机器学习课程

收获：

* 什么是机器学习：来自卡内基梅隆大学的Tom Mitchell提出：一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。
  
      监督学习：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程。
  
      无监督学习：根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。

* 线性回归模型
  
  ![](C:\Users\夹心\AppData\Roaming\marktext\images\2022-12-23-20-03-00-image.png)
  
  代价函数：J<sub>(θ0​,θ1​)​</sub>=1/2m ​∑<sup>m</sup><sub>i=1</sub>​(h<sub>θ</sub>​(x<sup>(i)</sup>)−y<sup>(i)</sup>)<sup>2</sup>
  
  在线性回归中，用等高线图表示代价函数J很美观；
  
  Batch梯度下降（全览整个训练集）： 每次梯度下降时，都要对J<sub>(θ0​,θ1​)​</sub>的偏导数进行一次计算，而J<sub>(θ0​,θ1​)​</sub>本身要遍历整个数据集，于是每次梯度下降也都会遍历整个数据集，所以这种梯度下降方法也称为：Batch（批，批量）梯度下降法。
  
  梯度下降是一种很广泛使用的方法。

* 多元梯度下降法
  
  <img src="file:///C:/Users/夹心/AppData/Roaming/marktext/images/2022-12-23-20-36-15-image.png" title="" alt="" width="226">
  
  * 特征缩放
  
  在特征数量很大时，梯度下降算法可能会计算得非常慢。而且在数据 x j  (j=1,2,⋯,n) 之间取值范围相差很大的情况下，梯度会变化得很快，不会朝着某一个方向比较平滑地收敛，而是会左右摇摆以至于计算次数增多。把特征数据处理之后，可以让梯度下降更快地收敛，提高算法效率。
  
  即改变特征的取值范围，使等高线图轮廓近似一个圆。
  
  * 均值归一化
    
    即减均值，再除以标准差，或简单地直接除以最大值与最小值的差，减少特征值取值范围对迭代的影响。

* 学习率
  
  1.如果学习率太小，会导致收敛得很慢。
  
  2.如果学习率太大，代价函数可能不是每次都会下降，可能不收敛。
  
  所以为了找到比较正确的学习率，最好画出代价函数的图形，根据图形来判断学习率的选择。可以每次使用3倍的增加学习率，会找到最大和最小的学习率，最后找到的值可能是比最大的学习率稍微小一点的值。

* 正规方程法可以不用特征缩放。其最佳的θ向量可由下式计算得出：
  
  <img src="file:///C:/Users/夹心/AppData/Roaming/marktext/images/2022-12-23-20-42-20-image.png" title="" alt="" width="185">

* 如何判断何时用正规方程法还是梯度下降法：
  
  梯度下降法：需要选择学习率α，需要迭代多次，n很大时仍可运行。
  
  正规方程法：不需要选择学习率α，不需要迭代多次，n很大时运行很慢。

* 向量化，让代码运行得更加高效。

* 在实现算法时，直接用Java可能会有些困难，可以先用高级一些的语言如octave（，python）实现成功之后，再把代码改写成Java

* Java BLAS【Basic Linear Algebra Subprograms，基础线性代数程序集】是一个抽象类，用于规范发布基础基础线性代数操作的数值库【常用于向量或矩阵计算】。该程序集最初发布于1979年，并用于创建更大的数值程序包【例如：LAPACK】。在高性能计算领域，BLAS被广泛使用。在原来的版本上，各大编程语言和各大算法公司都不断修改和完善设计原理。
  
  BLAS文档：
  
  https://spark.apache.org/docs/1.2.0/api/java/org/apache/spark/mllib/linalg/BLAS.html

* Logistic回归算法，预测值是离散值，是一种分类算法，Logistic回归也在一些文献中也称为logit回归、最大熵分类(MaxEnt)或对数线性分类器。sigmoid函数=logistic函数。
  
  1. 需要一个合适的分类函数来实现分类。可以使用单位阶跃函数或者Sigmoid函数。
  
  2. 用代价函数来表示预测值h(x)与实际值y的偏差(h−y)。要使得回归最佳拟合，那么偏差要尽可能小（偏差求和或取均值）。
  
  3. 记J(w,b)表示回归系数取w时的偏差，那么求最佳回归参数w,b就转换成了求J(w,b)的最小值。可以使用梯度下降法求回归参数w,b。
     
